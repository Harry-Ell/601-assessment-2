{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# value iteration "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This entire notebook was used during development steps, and only contains snippets of useful code. It will offer very little insight by reading it. It wont be commented, and I have only left it here since it is useful to take apart certain functions which appear in other parts of this repository. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from typing import Callable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_x, len_y = 10, 10\n",
    "\n",
    "states = [(i, j) for i in range(len_x) for j in range(len_y)]\n",
    "probabilities = {}\n",
    "\n",
    "corners = [(0, 0), (0, len_y - 1), (len_x - 1, 0), (len_x - 1, len_y - 1)]\n",
    "reward_states = [(8,7), (7,2), (3,4), (3,7)]\n",
    "reward_values = [10, 3, -5, -10]\n",
    "\n",
    "actions = [(1, 0),    # right\n",
    "           (-1, 0),   # left\n",
    "           (0, 1),    # up\n",
    "           (0, -1)]   # down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def populate_probabilities_rewards(state, actions):\n",
    "    sub_dirs = {}\n",
    "    for index, _ in enumerate(actions):\n",
    "        sub_dirs[index] = {}\n",
    "        for corner in corners:\n",
    "            sub_dirs[index][(corner)] = 0.25\n",
    "    return sub_dirs\n",
    "\n",
    "def populate_probabilities_usual(state, actions):\n",
    "    sub_dirs = {}\n",
    "    for index, intended_move in enumerate(actions):\n",
    "        sub_dirs[index] = {}\n",
    "        for actual_move in actions:\n",
    "            value = 0.7 if intended_move == actual_move else 0.1\n",
    "            sub_dirs[index][(state[0] + actual_move[0], state[1] + actual_move[1])] = value\n",
    "    return sub_dirs\n",
    "\n",
    "def populate_probabilities_edge(state, actions):\n",
    "    sub_dirs = {}\n",
    "\n",
    "    for index, intended_move in enumerate(actions):\n",
    "        sub_dirs[index] = {}\n",
    "\n",
    "        for actual_move in actions:\n",
    "            new_row = state[0] + actual_move[0]\n",
    "            new_col = state[1] + actual_move[1]\n",
    "\n",
    "            # Check if out of bounds\n",
    "            if (new_row < 0 or new_row >= len_x or\n",
    "                new_col < 0 or new_col >= len_y):\n",
    "                # Remain in the same cell \n",
    "                new_state = state\n",
    "            else:\n",
    "                new_state = (new_row, new_col)\n",
    "\n",
    "            value = 0.7 if intended_move == actual_move else 0.1\n",
    "            sub_dirs[index][new_state] = value\n",
    "\n",
    "    return sub_dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def populate_positive_rewards(state, actions, reward_value):\n",
    "    sub_dirs = {}\n",
    "    for index, _ in enumerate(actions):\n",
    "        sub_dirs[index] = {}\n",
    "        for corner in corners:\n",
    "            sub_dirs[index][(corner)] = reward_value\n",
    "    return sub_dirs\n",
    "\n",
    "def populate_negative_rewards(state, actions, reward_value):\n",
    "    sub_dirs = {}\n",
    "    for index, intended_move in enumerate(actions):\n",
    "        sub_dirs[index] = {}\n",
    "        for actual_move in actions:\n",
    "            sub_dirs[index][(state[0] + actual_move[0], state[1] + actual_move[1])] = reward_value\n",
    "    return sub_dirs\n",
    "\n",
    "def populate_edge_penalties(state, actions):\n",
    "    sub_dirs = {}\n",
    "\n",
    "    for index, intended_move in enumerate(actions):\n",
    "        sub_dirs[index] = {}\n",
    "\n",
    "        for actual_move in actions:\n",
    "            new_row = state[0] + actual_move[0]\n",
    "            new_col = state[1] + actual_move[1]\n",
    "\n",
    "            # Check if out of bounds\n",
    "            if (new_row < 0 or new_row >= len_x or\n",
    "                new_col < 0 or new_col >= len_y):\n",
    "                # Remain in the same cell \n",
    "                new_state = state\n",
    "            else:\n",
    "                new_state = (new_row, new_col)\n",
    "\n",
    "            value = -1 if new_state == state else 0\n",
    "            sub_dirs[index][new_state] = value\n",
    "\n",
    "    return sub_dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for state in states:\n",
    "    i, j = state\n",
    "    if state in reward_states and reward_values[reward_states.index(state)] > 0:\n",
    "        # teleportation step\n",
    "        probabilities[state] = populate_probabilities_rewards(state, actions)\n",
    "    elif 0 < i < len_x-1 and 0 < j < len_y-1:\n",
    "        # we are in standard operating conditions, make usual sub directories\n",
    "        probabilities[state] = populate_probabilities_usual(state, actions)\n",
    "    else: # we must be at an edge\n",
    "        probabilities[state] = populate_probabilities_edge(state, actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = {}\n",
    "for state in states:\n",
    "    i, j = state\n",
    "    if state in reward_states and reward_values[reward_states.index(state)] > 0:\n",
    "        # we are about to teleport\n",
    "        rewards[state] = populate_positive_rewards(state, actions, reward_values[reward_states.index(state)])\n",
    "    elif state in reward_states and reward_values[reward_states.index(state)] < 0:\n",
    "        # we get a reward but do not teleport\n",
    "        rewards[state] = populate_negative_rewards(state, actions, reward_values[reward_states.index(state)])\n",
    "    elif i in [0, len_x-1] or j in [0, len_y-1]:\n",
    "        rewards[state] = populate_edge_penalties(state, actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from MDP import GenericMDP\n",
    "import numpy as np\n",
    "mdp_solver = GenericMDP(states, actions, probabilities, rewards, 0.9, 200, len_x=len_x, len_y=len_y, reward_list=reward_states, reward_values= reward_values, problem_type='gridworld')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to probability = 0.050 for transition Healthy to Sick following action Relax\n",
      "Defaulting to probability = 0.300 for transition Healthy to Sick following action Party\n",
      "Defaulting to probability = 0.500 for transition Sick to Sick following action Relax\n",
      "Defaulting to probability = 0.900 for transition Sick to Sick following action Party\n"
     ]
    }
   ],
   "source": [
    "# we need to make the objects for rewards and actions for the states for ex 9.27\n",
    "states = ['Healthy', 'Sick']\n",
    "actions = ['Relax', 'Party']\n",
    "probabilities = {}\n",
    "if len(states) == 2:\n",
    "    for s in states: \n",
    "        temp = {}\n",
    "        for index, action in enumerate(actions):\n",
    "            temp2 = {}\n",
    "            probability = input(f'if in state = {s} and you take action = {action}, what is the probability of entering state = {states[0]}')\n",
    "            print(f'Defaulting to probability = {(1-float(probability.strip())):.3f} for transition {s} to {states[1]} following action {action}')\n",
    "            temp2[states[0]] = float(probability.strip())\n",
    "            temp2[states[1]] = (1-float(probability.strip()))\n",
    "            temp[index] = temp2\n",
    "        probabilities[s] = temp\n",
    "else:\n",
    "    for s in states: \n",
    "        temp = {}\n",
    "        for index, action in enumerate(actions):\n",
    "            temp2 = {}\n",
    "            for s_prime in states:\n",
    "                probability = input(f'if in state = {s} and you take action = {action}, what is the probability of entering state = {s_prime}')\n",
    "                temp2[s_prime] = float(probability.strip())\n",
    "            temp[index] = temp2\n",
    "        probabilities[s] = temp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next we do the rewards\n",
    "rewards = {}\n",
    "if len(actions) == 2:\n",
    "    for s in states: \n",
    "        temp = {}\n",
    "        for index, action in enumerate(actions):\n",
    "            temp2 = {}\n",
    "            reward = input(f'if in state = {s} and you take action = {action}, what is the reward given?')\n",
    "            \n",
    "            temp2[states[0]] = float(reward)\n",
    "            temp2[states[1]] = float(reward)\n",
    "            temp[index] = temp2\n",
    "        rewards[s] = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
