{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## generic value iteration algo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from typing import Callable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SPECIALISED FOR EX 9.27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We determine the optimal policy to be:\n",
      "if healthy, then Party\n",
      "if sick, then Relax\n"
     ]
    }
   ],
   "source": [
    "def Value_iteration(S:list[tuple], # states\n",
    "                    A:tuple,       # actions\n",
    "                    P:list,        # probabilities\n",
    "                    R:list,         # rewards\n",
    "                    cutoff:int = 1000, \n",
    "                    discount_rate:float=0.8):\n",
    "    # we assign V0[s] arbitrarily to start with\n",
    "    V0 = np.zeros(2) #np.random.rand(len(S))  # Ensures values are float\n",
    "    V_k = V0.astype(float)  \n",
    "    k = 0\n",
    "    pi = [0,0]\n",
    "    while k < cutoff:\n",
    "        V_k_minus_1 = V_k\n",
    "        for index, state in enumerate(S):\n",
    "            possible_values = [R[index][a] + discount_rate * (P[index][a]*V_k_minus_1[0] + (1-P[index][a])*V_k_minus_1[1]) for a in range(len(A))]\n",
    "            V_k[index] = max(possible_values).astype(float)\n",
    "        k += 1\n",
    "\n",
    "    for index, state in enumerate(S):\n",
    "        possible_values =  [R[index][a] + discount_rate * (P[index][a]*V_k_minus_1[0] + (1-P[index][a])*V_k_minus_1[1]) for a in range(len(A))]\n",
    "        pi[index] =  possible_values.index(max(possible_values))\n",
    "\n",
    "    return V_k, pi\n",
    "\n",
    "\n",
    "###############################################\n",
    "############# FUNCTION CALL ###################\n",
    "###############################################\n",
    "\n",
    "States = ('healthy', 'sick')\n",
    "Actions = ('Relax', 'Party')\n",
    "# we will be taking 0 to be healthy and 1 to be sick in terms of state\n",
    "# and we will be using 0 for relax, and 1 to party \n",
    "\n",
    "Probabilities = [[0.95, # if healthy and relax\n",
    "                 0.7],  # if healthy and party\n",
    "                [ 0.5,  # if sick and relax\n",
    "                 0.1]]  # if sick and party\n",
    "\n",
    "Rewards = [[7, # if healthy and relax\n",
    "            10],  # if healthy and party\n",
    "            [0,  # if sick and relax\n",
    "            2]]  # if sick and party\n",
    "\n",
    "values, policies = Value_iteration(States, Actions, Probabilities, Rewards)\n",
    "\n",
    "print('We determine the optimal policy to be:')\n",
    "for index, value in enumerate(policies):\n",
    "    print(f'if {States[index]}, then {Actions[value]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SPECIALISED FOR EX 9.28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bellman_equation(actions, state, probabilities, V, rewards, discount_factor):\n",
    "    values = []\n",
    "    i, j = state  # coords\n",
    "    corners = [(0, 0), (0, len(V) - 1), (len(V) - 1, 0), (len(V) - 1, len(V) - 1)]\n",
    "    \n",
    "    for intended_action in actions:\n",
    "        # this inner for loop should be maximising over all intended actions to tell you which is the best\n",
    "        summation_term = 0\n",
    "        \n",
    "        for possible_action in actions:\n",
    "            probability_of_occurence = probabilities[0] if possible_action == intended_action else probabilities[1]\n",
    "            \n",
    "            # check where we are, and if this is within the grid\n",
    "            new_i, new_j = i + possible_action[0], j + possible_action[1]\n",
    "            \n",
    "            if 0 <= new_i < len(V) and 0 <= new_j < len(V[0]):\n",
    "                reward = rewards[i, j]  # Immediate reward \n",
    "                \n",
    "                if reward != 0:  # if you are on a reward cell/ move into one, teleport to random corner\n",
    "                    future_value = sum(V[corner] for corner in corners) / 4  \n",
    "                else:\n",
    "                    future_value = V[new_i, new_j]  # non special transition\n",
    "            else:\n",
    "                # stay in place and apply wall penalty if you hit wall \n",
    "                # print('this is triggered')\n",
    "                reward = -1\n",
    "                future_value = V[i, j]\n",
    "            summation_term += probability_of_occurence * (reward + discount_factor * future_value)\n",
    "        \n",
    "        values.append(summation_term)  \n",
    "    \n",
    "    return max(values) # only return max value, not all of them \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Value_iteration(S:np.ndarray, # states\n",
    "                    A:tuple,       # actions\n",
    "                    P:list,        # probabilities\n",
    "                    R:list,         # rewards\n",
    "                    cutoff:int = 1000, \n",
    "                    discount_rate:float=0.8):\n",
    "    \n",
    "    # we assign V0[s] arbitrarily to start with\n",
    "    V0 = np.zeros(S.shape) \n",
    "    V_k = V0.astype(float)  \n",
    "    k = 0\n",
    "    pi = np.zeros(S.shape) \n",
    "    while k < cutoff:\n",
    "        V_k_minus_1 = V_k.copy()\n",
    "        \n",
    "        for (i, j), value in np.ndenumerate(S):\n",
    "            V_k[i][j] = bellman_equation(A, (i,j), P, V_k_minus_1, R, discount_rate)\n",
    "        k += 1\n",
    "\n",
    "    return np.round(V_k, decimals=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = np.zeros((10, 10))\n",
    "\n",
    "actions = [(1, 0),    # right\n",
    "           (-1, 0),   # left\n",
    "           (0, 1),    # up\n",
    "           (0, -1)]   # down\n",
    "\n",
    "probabilities = [0.7, 0.1]\n",
    "\n",
    "rewards = np.zeros((10,10))\n",
    "rewards[8][7] = 10\n",
    "rewards[7][2] = 3\n",
    "rewards[3][4] = -5\n",
    "rewards[3][7] = -10\n",
    "\n",
    "output = Value_iteration(S = states, \n",
    "                A = actions, \n",
    "                P = probabilities, \n",
    "                R = rewards, \n",
    "                cutoff=1000,\n",
    "                discount_rate=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.9  1.4  1.7  2.   2.5  2.9  3.5  4.   3.6  3. ]\n",
      " [ 1.3  1.7  2.1  2.5  3.   3.6  4.2  4.9  4.3  3.7]\n",
      " [ 1.5  1.9  2.3  2.8  3.4  4.1  5.   6.   5.1  4.5]\n",
      " [ 1.4  1.8  2.1  2.4  3.4  4.1  4.8  5.6  6.   5.4]\n",
      " [ 1.4  1.8  1.8 -2.   3.5  4.6  5.5  6.4  7.2  6.5]\n",
      " [ 1.7  2.1  2.6  3.2  4.5  5.5  6.5  7.6  8.7  7.7]\n",
      " [ 1.6  2.1  2.5  3.1  5.1  6.4  7.6  9.  10.6  9.1]\n",
      " [ 1.5  1.8  1.4 -7.   4.7  7.   8.7 10.6 13.  10.7]\n",
      " [ 1.7  2.1  2.6  3.2  5.1  6.4  7.6  9.  10.6  9.1]\n",
      " [ 1.7  2.3  2.9  3.6  4.5  5.4  6.5  7.6  8.8  7.7]]\n"
     ]
    }
   ],
   "source": [
    "print(output.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "V0 = np.zeros(states.shape) \n",
    "V = V0.astype(float)  \n",
    "discount_factor = 0.9\n",
    "values = []\n",
    "i, j = (8,7)\n",
    "\n",
    "corners = [(0, 0), (0, len(V) - 1), (len(V) - 1, 0), (len(V) - 1, len(V) - 1)]\n",
    "\n",
    "for intended_action in actions:\n",
    "    # this inner for loop should be maximising over all intended actions to tell you which is the best\n",
    "    summation_term = 0\n",
    "    \n",
    "    for possible_action in actions:\n",
    "        probability_of_occurence = probabilities[0] if possible_action == intended_action else probabilities[1]\n",
    "        \n",
    "        # check where we are, and if this is within the grid\n",
    "        new_i, new_j = i + possible_action[0], j + possible_action[1]\n",
    "        \n",
    "        if 0 <= new_i < len(V) and 0 <= new_j < len(V[0]):\n",
    "            reward = rewards[i, j]  # Immediate reward \n",
    "            \n",
    "            if reward != 0:  # if you are on a reward cell/ move into one, teleport to random corner\n",
    "                future_value = sum(V[corner] for corner in corners) / 4  \n",
    "            else:\n",
    "                future_value = V[new_i, new_j]  # non special transition\n",
    "        else:\n",
    "            # stay in place and apply wall penalty if you hit wall \n",
    "            # print('this is triggered')\n",
    "            reward = -1\n",
    "            future_value = V[i, j]\n",
    "        summation_term += probability_of_occurence * (reward + discount_factor * future_value)\n",
    "    \n",
    "    values.append(summation_term)  \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def populate_probabilities_rewards(state, actions):\n",
    "    sub_dirs = {}\n",
    "    for index, _ in enumerate(actions):\n",
    "        sub_dirs[index] = {}\n",
    "        for corner in corners:\n",
    "            sub_dirs[index][(corner)] = 0.25\n",
    "    return sub_dirs\n",
    "\n",
    "def populate_probabilities_usual(state, actions):\n",
    "    sub_dirs = {}\n",
    "    for index, intended_move in enumerate(actions):\n",
    "        sub_dirs[index] = {}\n",
    "        for actual_move in actions:\n",
    "            value = 0.7 if intended_move == actual_move else 0.1\n",
    "            sub_dirs[index][(state[0] + actual_move[0], state[1] + actual_move[1])] = value\n",
    "    return sub_dirs\n",
    "\n",
    "def populate_probabilities_edge(state, actions):\n",
    "    sub_dirs = {}\n",
    "\n",
    "    for index, intended_move in enumerate(actions):\n",
    "        sub_dirs[index] = {}\n",
    "\n",
    "        for actual_move in actions:\n",
    "            new_row = state[0] + actual_move[0]\n",
    "            new_col = state[1] + actual_move[1]\n",
    "\n",
    "            # Check if out of bounds\n",
    "            if (new_row < 0 or new_row >= len_x or\n",
    "                new_col < 0 or new_col >= len_y):\n",
    "                # Remain in the same cell \n",
    "                new_state = state\n",
    "            else:\n",
    "                new_state = (new_row, new_col)\n",
    "\n",
    "            value = 0.7 if intended_move == actual_move else 0.1\n",
    "            sub_dirs[index][new_state] = value\n",
    "\n",
    "    return sub_dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def populate_positive_rewards(state, actions, reward_value):\n",
    "    sub_dirs = {}\n",
    "    for index, _ in enumerate(actions):\n",
    "        sub_dirs[index] = {}\n",
    "        for corner in corners:\n",
    "            sub_dirs[index][(corner)] = reward_value\n",
    "    return sub_dirs\n",
    "\n",
    "def populate_negative_rewards(state, actions, reward_value):\n",
    "    sub_dirs = {}\n",
    "    for index, intended_move in enumerate(actions):\n",
    "        sub_dirs[index] = {}\n",
    "        for actual_move in actions:\n",
    "            sub_dirs[index][(state[0] + actual_move[0], state[1] + actual_move[1])] = reward_value\n",
    "    return sub_dirs\n",
    "\n",
    "def populate_edge_penalties(state, actions):\n",
    "    sub_dirs = {}\n",
    "\n",
    "    for index, intended_move in enumerate(actions):\n",
    "        sub_dirs[index] = {}\n",
    "\n",
    "        for actual_move in actions:\n",
    "            new_row = state[0] + actual_move[0]\n",
    "            new_col = state[1] + actual_move[1]\n",
    "\n",
    "            # Check if out of bounds\n",
    "            if (new_row < 0 or new_row >= len_x or\n",
    "                new_col < 0 or new_col >= len_y):\n",
    "                # Remain in the same cell \n",
    "                new_state = state\n",
    "            else:\n",
    "                new_state = (new_row, new_col)\n",
    "\n",
    "            value = -1 if new_state == state else 0\n",
    "            sub_dirs[index][new_state] = value\n",
    "\n",
    "    return sub_dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_x, len_y = 15, 20\n",
    "\n",
    "states = [(i, j) for i in range(len_x) for j in range(len_y)]\n",
    "probabilities = {}\n",
    "\n",
    "corners = [(0, 0), (0, len_y - 1), (len_x - 1, 0), (len_x - 1, len_y - 1)]\n",
    "reward_states = [(8,7), (7,2), (3,4), (3,7), (13,4)]\n",
    "reward_values = [10, 3, -5, -10, 10]\n",
    "\n",
    "actions = [(1, 0),    # right\n",
    "           (-1, 0),   # left\n",
    "           (0, 1),    # up\n",
    "           (0, -1)]   # down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for state in states:\n",
    "    i, j = state\n",
    "    if state in reward_states and reward_values[reward_states.index(state)] > 0:\n",
    "        # teleportation step\n",
    "        probabilities[state] = populate_probabilities_rewards(state, actions)\n",
    "    elif 0 < i < len_x-1 and 0 < j < len_y-1:\n",
    "        # we are in standard operating conditions, make usual sub directories\n",
    "        probabilities[state] = populate_probabilities_usual(state, actions)\n",
    "    else: # we must be at an edge\n",
    "        probabilities[state] = populate_probabilities_edge(state, actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = {}\n",
    "for state in states:\n",
    "    i, j = state\n",
    "    if state in reward_states and reward_values[reward_states.index(state)] > 0:\n",
    "        # we are about to teleport\n",
    "        rewards[state] = populate_positive_rewards(state, actions, reward_values[reward_states.index(state)])\n",
    "    elif state in reward_states and reward_values[reward_states.index(state)] < 0:\n",
    "        # we get a reward but do not teleport\n",
    "        rewards[state] = populate_negative_rewards(state, actions, reward_values[reward_states.index(state)])\n",
    "    elif i in [0, len_x-1] or j in [0, len_y-1]:\n",
    "        rewards[state] = populate_edge_penalties(state, actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from MDP import GenericMDP\n",
    "import numpy as np\n",
    "#mdp_solver = GenericMDP(states, actions, probabilities, rewards, 0.9, 200, len_x=len_x, len_y=len_y, reward_list=reward_states, reward_values= reward_values, problem_type='gridworld')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: ''",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 22\u001b[39m\n\u001b[32m     20\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m s_prime \u001b[38;5;129;01min\u001b[39;00m states:\n\u001b[32m     21\u001b[39m         probability = \u001b[38;5;28minput\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mif in state = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00ms\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m and you take action = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maction\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, what is the probability of entering state = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00ms_prime\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m         temp2[s_prime] = \u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprobability\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstrip\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m     temp[index] = temp2\n\u001b[32m     24\u001b[39m probabilities[s] = temp\n",
      "\u001b[31mValueError\u001b[39m: could not convert string to float: ''"
     ]
    }
   ],
   "source": [
    "# we need to make the objects for rewards and actions for the states for ex 9.27\n",
    "states = ['Healthy', 'Sick', 'Dead']\n",
    "actions = ['Relax', 'Party']\n",
    "if len(states) == 2:\n",
    "    for s in states: \n",
    "        temp = {}\n",
    "        for index, action in enumerate(actions):\n",
    "            temp2 = {}\n",
    "            probability = input(f'if in state = {s} and you take action = {action}, what is the probability of entering state = {states[0]}')\n",
    "            print(f'Defaulting to probability = {(1-float(probability.strip())):.3f} for transition {s} to {states[1]} following action {action}')\n",
    "            temp2[states[0]] = float(probability.strip())\n",
    "            temp2[states[1]] = (1-float(probability.strip()))\n",
    "            temp[index] = temp2\n",
    "        probabilities[s] = temp\n",
    "else:\n",
    "    for s in states: \n",
    "        temp = {}\n",
    "        for index, action in enumerate(actions):\n",
    "            temp2 = {}\n",
    "            for s_prime in states:\n",
    "                probability = input(f'if in state = {s} and you take action = {action}, what is the probability of entering state = {s_prime}')\n",
    "                temp2[s_prime] = float(probability.strip())\n",
    "            temp[index] = temp2\n",
    "        probabilities[s] = temp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next we do the rewards\n",
    "rewards = {}\n",
    "if len(actions) == 2:\n",
    "    for s in states: \n",
    "        temp = {}\n",
    "        for index, action in enumerate(actions):\n",
    "            temp2 = {}\n",
    "            reward = input(f'if in state = {s} and you take action = {action}, what is the reward given?')\n",
    "            \n",
    "            temp2[states[0]] = float(reward)\n",
    "            temp2[states[1]] = float(reward)\n",
    "            temp[index] = temp2\n",
    "        rewards[s] = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "if in state Healthy, optimal action is Party\n",
      "if in state Sick, optimal action is Relax\n"
     ]
    }
   ],
   "source": [
    "mdp_solver = GenericMDP(states, actions, probabilities, rewards, 0.8, 200)\n",
    "policy, values = mdp_solver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Healthy': np.float64(35.714285714285694),\n",
       " 'Sick': np.float64(23.809523809523796)}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
