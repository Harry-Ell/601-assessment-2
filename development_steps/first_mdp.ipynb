{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## generic value iteration algo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from typing import Callable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SPECIALISED FOR EX 9.27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We determine the optimal policy to be:\n",
      "if healthy, then Party\n",
      "if sick, then Relax\n"
     ]
    }
   ],
   "source": [
    "def Value_iteration(S:list[tuple], # states\n",
    "                    A:tuple,       # actions\n",
    "                    P:list,        # probabilities\n",
    "                    R:list,         # rewards\n",
    "                    cutoff:int = 1000, \n",
    "                    discount_rate:float=0.8):\n",
    "    # we assign V0[s] arbitrarily to start with\n",
    "    V0 = np.zeros(2) #np.random.rand(len(S))  # Ensures values are float\n",
    "    V_k = V0.astype(float)  \n",
    "    k = 0\n",
    "    pi = [0,0]\n",
    "    while k < cutoff:\n",
    "        V_k_minus_1 = V_k\n",
    "        for index, state in enumerate(S):\n",
    "            possible_values = [R[index][a] + discount_rate * (P[index][a]*V_k_minus_1[0] + (1-P[index][a])*V_k_minus_1[1]) for a in range(len(A))]\n",
    "            V_k[index] = max(possible_values).astype(float)\n",
    "        k += 1\n",
    "\n",
    "    for index, state in enumerate(S):\n",
    "        possible_values =  [R[index][a] + discount_rate * (P[index][a]*V_k_minus_1[0] + (1-P[index][a])*V_k_minus_1[1]) for a in range(len(A))]\n",
    "        pi[index] =  possible_values.index(max(possible_values))\n",
    "\n",
    "    return V_k, pi\n",
    "\n",
    "\n",
    "###############################################\n",
    "############# FUNCTION CALL ###################\n",
    "###############################################\n",
    "\n",
    "States = ('healthy', 'sick')\n",
    "Actions = ('Relax', 'Party')\n",
    "# we will be taking 0 to be healthy and 1 to be sick in terms of state\n",
    "# and we will be using 0 for relax, and 1 to party \n",
    "\n",
    "Probabilities = [[0.95, # if healthy and relax\n",
    "                 0.7],  # if healthy and party\n",
    "                [ 0.5,  # if sick and relax\n",
    "                 0.1]]  # if sick and party\n",
    "\n",
    "Rewards = [[7, # if healthy and relax\n",
    "            10],  # if healthy and party\n",
    "            [0,  # if sick and relax\n",
    "            2]]  # if sick and party\n",
    "\n",
    "values, policies = Value_iteration(States, Actions, Probabilities, Rewards)\n",
    "\n",
    "print('We determine the optimal policy to be:')\n",
    "for index, value in enumerate(policies):\n",
    "    print(f'if {States[index]}, then {Actions[value]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SPECIALISED FOR EX 9.28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bellman_equation(actions, state, probabilities, V, rewards, discount_factor):\n",
    "    values = []\n",
    "    i, j = state  # coords\n",
    "    corners = [(0, 0), (0, len(V) - 1), (len(V) - 1, 0), (len(V) - 1, len(V) - 1)]\n",
    "    \n",
    "    for intended_action in actions:\n",
    "        # this inner for loop should be maximising over all intended actions to tell you which is the best\n",
    "        summation_term = 0\n",
    "        \n",
    "        for possible_action in actions:\n",
    "            probability_of_occurence = probabilities[0] if possible_action == intended_action else probabilities[1]\n",
    "            \n",
    "            # check where we are, and if this is within the grid\n",
    "            new_i, new_j = i + possible_action[0], j + possible_action[1]\n",
    "            \n",
    "            if 0 <= new_i < len(V) and 0 <= new_j < len(V[0]):\n",
    "                reward = rewards[new_i, new_j]  # Immediate reward \n",
    "                \n",
    "                if reward != 0:  # if you are on a reward cell/ move into one, teleport to random corner\n",
    "                    future_value = sum(V[corner] for corner in corners) / 4  \n",
    "                else:\n",
    "                    future_value = V[new_i, new_j]  # non special transition\n",
    "            else:\n",
    "                # stay in place and apply wall penalty if you hit wall \n",
    "                # print('this is triggered')\n",
    "                reward = -1\n",
    "                future_value = V[i, j]\n",
    "            summation_term += probability_of_occurence * (reward + discount_factor * future_value)\n",
    "        \n",
    "        values.append(summation_term)  \n",
    "    \n",
    "    return max(values) # only return max value, not all of them \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Value_iteration(S:np.ndarray, # states\n",
    "                    A:tuple,       # actions\n",
    "                    P:list,        # probabilities\n",
    "                    R:list,         # rewards\n",
    "                    cutoff:int = 1000, \n",
    "                    discount_rate:float=0.8):\n",
    "    # we assign V0[s] arbitrarily to start with\n",
    "    V0 = np.zeros(S.shape) \n",
    "    V_k = V0.astype(float)  \n",
    "    k = 0\n",
    "    pi = np.zeros(S.shape) \n",
    "    while k < cutoff:\n",
    "        V_k_minus_1 = V_k\n",
    "        for (i, j), value in np.ndenumerate(S):\n",
    "            V_k[i][j] = bellman_equation(A, (i,j), P, V_k_minus_1, R, 0.9)\n",
    "        k += 1\n",
    "\n",
    "    return np.round(V_k, decimals=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = np.zeros((10, 10))\n",
    "\n",
    "actions = [(1, 0),    # right\n",
    "           (-1, 0),   # left\n",
    "           (0, 1),    # up\n",
    "           (0, -1)]   # down\n",
    "\n",
    "probabilities = [0.7, 0.1]\n",
    "\n",
    "rewards = np.zeros((10,10))\n",
    "rewards[8][7] = 10\n",
    "rewards[7][2] = 3\n",
    "rewards[3][4] = -5\n",
    "rewards[3][7] = -10\n",
    "\n",
    "output = Value_iteration(S = states, \n",
    "                A = actions, \n",
    "                P = probabilities, \n",
    "                R = rewards, \n",
    "                cutoff=1,\n",
    "                discount_rate=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.2  -0.12 -0.11 -0.11 -0.11 -0.11 -0.11 -0.11 -0.11 -0.21]\n",
      " [-0.12 -0.02 -0.01 -0.01 -0.01 -0.01 -0.01  2.02  1.27  0.68]\n",
      " [-0.11 -0.01 -0.   -0.   -0.   -0.    2.03  1.46  2.15  1.32]\n",
      " [-0.11 -0.01 -0.   -0.51 -0.05 -0.    1.28  2.15  1.55  0.99]\n",
      " [-0.11 -0.01 -0.51 -0.09 -0.51 -0.05  0.8   1.43  1.1   0.68]\n",
      " [-0.11 -0.01 -0.05 -0.51 -0.09 -0.01  0.5   0.94  0.78  0.45]\n",
      " [-0.11 -0.01 -0.01 -1.06 -0.1  -0.01  0.32  0.62  7.06  4.39]\n",
      " [-0.11 -0.01 -1.01 -0.19 -1.02 -0.09  0.19  7.01  5.08  7.2 ]\n",
      " [-0.11 -0.01 -0.09 -1.02 -0.18 -0.02  0.12  4.43  7.33  5.17]\n",
      " [-0.21 -0.12 -0.12 -0.2  -0.13 -0.11 -0.04  2.69  4.76  3.48]]\n"
     ]
    }
   ],
   "source": [
    "print(output.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
