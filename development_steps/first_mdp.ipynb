{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## generic value iteration algo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from typing import Callable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SPECIALISED FOR EX 9.27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We determine the optimal policy to be:\n",
      "if healthy, then Party\n",
      "if sick, then Relax\n"
     ]
    }
   ],
   "source": [
    "def Value_iteration(S:list[tuple], # states\n",
    "                    A:tuple,       # actions\n",
    "                    P:list,        # probabilities\n",
    "                    R:list,         # rewards\n",
    "                    cutoff:int = 1000, \n",
    "                    discount_rate:float=0.8):\n",
    "    # we assign V0[s] arbitrarily to start with\n",
    "    V0 = np.zeros(2) #np.random.rand(len(S))  # Ensures values are float\n",
    "    V_k = V0.astype(float)  \n",
    "    k = 0\n",
    "    pi = [0,0]\n",
    "    while k < cutoff:\n",
    "        V_k_minus_1 = V_k\n",
    "        for index, state in enumerate(S):\n",
    "            possible_values = [R[index][a] + discount_rate * (P[index][a]*V_k_minus_1[0] + (1-P[index][a])*V_k_minus_1[1]) for a in range(len(A))]\n",
    "            V_k[index] = max(possible_values).astype(float)\n",
    "        k += 1\n",
    "\n",
    "    for index, state in enumerate(S):\n",
    "        possible_values =  [R[index][a] + discount_rate * (P[index][a]*V_k_minus_1[0] + (1-P[index][a])*V_k_minus_1[1]) for a in range(len(A))]\n",
    "        pi[index] =  possible_values.index(max(possible_values))\n",
    "\n",
    "    return V_k, pi\n",
    "\n",
    "\n",
    "###############################################\n",
    "############# FUNCTION CALL ###################\n",
    "###############################################\n",
    "\n",
    "States = ('healthy', 'sick')\n",
    "Actions = ('Relax', 'Party')\n",
    "# we will be taking 0 to be healthy and 1 to be sick in terms of state\n",
    "# and we will be using 0 for relax, and 1 to party \n",
    "\n",
    "Probabilities = [[0.95, # if healthy and relax\n",
    "                 0.7],  # if healthy and party\n",
    "                [ 0.5,  # if sick and relax\n",
    "                 0.1]]  # if sick and party\n",
    "\n",
    "Rewards = [[7, # if healthy and relax\n",
    "            10],  # if healthy and party\n",
    "            [0,  # if sick and relax\n",
    "            2]]  # if sick and party\n",
    "\n",
    "values, policies = Value_iteration(States, Actions, Probabilities, Rewards)\n",
    "\n",
    "print('We determine the optimal policy to be:')\n",
    "for index, value in enumerate(policies):\n",
    "    print(f'if {States[index]}, then {Actions[value]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SPECIALISED FOR EX 9.28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bellman_equation(actions, state, probabilities, V, rewards, discount_factor):\n",
    "    values = []\n",
    "    i, j = state  # coords\n",
    "    corners = [(0, 0), (0, len(V) - 1), (len(V) - 1, 0), (len(V) - 1, len(V) - 1)]\n",
    "    \n",
    "    for intended_action in actions:\n",
    "        # this inner for loop should be maximising over all intended actions to tell you which is the best\n",
    "        summation_term = 0\n",
    "        \n",
    "        for possible_action in actions:\n",
    "            probability_of_occurence = probabilities[0] if possible_action == intended_action else probabilities[1]\n",
    "            \n",
    "            # check where we are, and if this is within the grid\n",
    "            new_i, new_j = i + possible_action[0], j + possible_action[1]\n",
    "            \n",
    "            if 0 <= new_i < len(V) and 0 <= new_j < len(V[0]):\n",
    "                reward = rewards[i, j]  # Immediate reward \n",
    "                \n",
    "                if reward != 0:  # if you are on a reward cell/ move into one, teleport to random corner\n",
    "                    future_value = sum(V[corner] for corner in corners) / 4  \n",
    "                else:\n",
    "                    future_value = V[new_i, new_j]  # non special transition\n",
    "            else:\n",
    "                # stay in place and apply wall penalty if you hit wall \n",
    "                # print('this is triggered')\n",
    "                reward = -1\n",
    "                future_value = V[i, j]\n",
    "            summation_term += probability_of_occurence * (reward + discount_factor * future_value)\n",
    "        \n",
    "        values.append(summation_term)  \n",
    "    \n",
    "    return max(values) # only return max value, not all of them \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Value_iteration(S:np.ndarray, # states\n",
    "                    A:tuple,       # actions\n",
    "                    P:list,        # probabilities\n",
    "                    R:list,         # rewards\n",
    "                    cutoff:int = 1000, \n",
    "                    discount_rate:float=0.8):\n",
    "    \n",
    "    # we assign V0[s] arbitrarily to start with\n",
    "    V0 = np.zeros(S.shape) \n",
    "    V_k = V0.astype(float)  \n",
    "    k = 0\n",
    "    pi = np.zeros(S.shape) \n",
    "    while k < cutoff:\n",
    "        V_k_minus_1 = V_k.copy()\n",
    "        \n",
    "        for (i, j), value in np.ndenumerate(S):\n",
    "            V_k[i][j] = bellman_equation(A, (i,j), P, V_k_minus_1, R, discount_rate)\n",
    "        k += 1\n",
    "\n",
    "    return np.round(V_k, decimals=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = np.zeros((10, 10))\n",
    "\n",
    "actions = [(1, 0),    # right\n",
    "           (-1, 0),   # left\n",
    "           (0, 1),    # up\n",
    "           (0, -1)]   # down\n",
    "\n",
    "probabilities = [0.7, 0.1]\n",
    "\n",
    "rewards = np.zeros((10,10))\n",
    "rewards[8][7] = 10\n",
    "rewards[7][2] = 3\n",
    "rewards[3][4] = -5\n",
    "rewards[3][7] = -10\n",
    "\n",
    "output = Value_iteration(S = states, \n",
    "                A = actions, \n",
    "                P = probabilities, \n",
    "                R = rewards, \n",
    "                cutoff=1000,\n",
    "                discount_rate=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.9  1.4  1.7  2.   2.5  2.9  3.5  4.   3.6  3. ]\n",
      " [ 1.3  1.7  2.1  2.5  3.   3.6  4.2  4.9  4.3  3.7]\n",
      " [ 1.5  1.9  2.3  2.8  3.4  4.1  5.   6.   5.1  4.5]\n",
      " [ 1.4  1.8  2.1  2.4  3.4  4.1  4.8  5.6  6.   5.4]\n",
      " [ 1.4  1.8  1.8 -2.   3.5  4.6  5.5  6.4  7.2  6.5]\n",
      " [ 1.7  2.1  2.6  3.2  4.5  5.5  6.5  7.6  8.7  7.7]\n",
      " [ 1.6  2.1  2.5  3.1  5.1  6.4  7.6  9.  10.6  9.1]\n",
      " [ 1.5  1.8  1.4 -7.   4.7  7.   8.7 10.6 13.  10.7]\n",
      " [ 1.7  2.1  2.6  3.2  5.1  6.4  7.6  9.  10.6  9.1]\n",
      " [ 1.7  2.3  2.9  3.6  4.5  5.4  6.5  7.6  8.8  7.7]]\n"
     ]
    }
   ],
   "source": [
    "print(output.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "V0 = np.zeros(states.shape) \n",
    "V = V0.astype(float)  \n",
    "discount_factor = 0.9\n",
    "values = []\n",
    "i, j = (8,7)\n",
    "\n",
    "corners = [(0, 0), (0, len(V) - 1), (len(V) - 1, 0), (len(V) - 1, len(V) - 1)]\n",
    "\n",
    "for intended_action in actions:\n",
    "    # this inner for loop should be maximising over all intended actions to tell you which is the best\n",
    "    summation_term = 0\n",
    "    \n",
    "    for possible_action in actions:\n",
    "        probability_of_occurence = probabilities[0] if possible_action == intended_action else probabilities[1]\n",
    "        \n",
    "        # check where we are, and if this is within the grid\n",
    "        new_i, new_j = i + possible_action[0], j + possible_action[1]\n",
    "        \n",
    "        if 0 <= new_i < len(V) and 0 <= new_j < len(V[0]):\n",
    "            reward = rewards[i, j]  # Immediate reward \n",
    "            \n",
    "            if reward != 0:  # if you are on a reward cell/ move into one, teleport to random corner\n",
    "                future_value = sum(V[corner] for corner in corners) / 4  \n",
    "            else:\n",
    "                future_value = V[new_i, new_j]  # non special transition\n",
    "        else:\n",
    "            # stay in place and apply wall penalty if you hit wall \n",
    "            # print('this is triggered')\n",
    "            reward = -1\n",
    "            future_value = V[i, j]\n",
    "        summation_term += probability_of_occurence * (reward + discount_factor * future_value)\n",
    "    \n",
    "    values.append(summation_term)  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_x, len_y = 10, 10 \n",
    "\n",
    "states = [(i, j) for i in range(len_x) for j in range(len_y)]\n",
    "probabilities = {}\n",
    "\n",
    "corners = [(0, 0), (0, len_y - 1), (len_x - 1, 0), (len_x - 1, len_y - 1)]\n",
    "reward_states = [(8,7), (7,2), (3,4), (3,7)]\n",
    "reward_values = [10, 3, -5, -10]\n",
    "\n",
    "actions = [(1, 0),    # right\n",
    "           (-1, 0),   # left\n",
    "           (0, 1),    # up\n",
    "           (0, -1)]   # down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def populate_probabilities_usual(state, actions):\n",
    "    sub_dirs = {}\n",
    "    for index, intended_move in enumerate(actions):\n",
    "        sub_dirs[index] = {}\n",
    "        for actual_move in actions:\n",
    "            value = 0.7 if intended_move == actual_move else 0.1\n",
    "            sub_dirs[index][(state[0] + actual_move[0], state[1] + actual_move[1])] = value\n",
    "    return sub_dirs\n",
    "\n",
    "def populate_probabilities_rewards(state, actions):\n",
    "    sub_dirs = {}\n",
    "    for index, _ in enumerate(actions):\n",
    "        sub_dirs[index] = {}\n",
    "        for corner in corners:\n",
    "            sub_dirs[index][(corner)] = 0.25\n",
    "    return sub_dirs\n",
    "\n",
    "def populate_probabilities_edge(state, actions):\n",
    "    sub_dirs = {}\n",
    "\n",
    "    for index, intended_move in enumerate(actions):\n",
    "        sub_dirs[index] = {}\n",
    "\n",
    "        for actual_move in actions:\n",
    "            new_row = state[0] + actual_move[0]\n",
    "            new_col = state[1] + actual_move[1]\n",
    "\n",
    "            # Check if out of bounds\n",
    "            if (new_row < 0 or new_row >= len_y or\n",
    "                new_col < 0 or new_col >= len_x):\n",
    "                # Remain in the same cell \n",
    "                new_state = state\n",
    "            else:\n",
    "                new_state = (new_row, new_col)\n",
    "\n",
    "            value = 0.7 if intended_move == actual_move else 0.1\n",
    "            sub_dirs[index][new_state] = value\n",
    "\n",
    "    return sub_dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for state in states:\n",
    "    i, j = state\n",
    "    if state in reward_states and reward_values[reward_states.index(state)]:\n",
    "        probabilities[state] = populate_probabilities_rewards(state, actions)\n",
    "    elif 0 < i < len_x-1 and 0 < j < len_y-1:\n",
    "        # we are in standard operating conditions, make usual sub directories\n",
    "        probabilities[state] = populate_probabilities_usual(state, actions)\n",
    "    else: # we must be at an edge\n",
    "        probabilities[state] = populate_probabilities_edge(state, actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {(9, 7): 0.7, (8, 7): 0.1, (9, 8): 0.1, (9, 6): 0.1},\n",
       " 1: {(9, 7): 0.1, (8, 7): 0.7, (9, 8): 0.1, (9, 6): 0.1},\n",
       " 2: {(9, 7): 0.1, (8, 7): 0.1, (9, 8): 0.7, (9, 6): 0.1},\n",
       " 3: {(9, 7): 0.1, (8, 7): 0.1, (9, 8): 0.1, (9, 6): 0.7}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probabilities[(9,7)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next, we have to populate the rewards array. this will be another dict except only 2 layered\n",
    "def populate_reward_cells(actions, reward_states, reward_values):\n",
    "    rewards = {}\n",
    "    temp_dict = {}\n",
    "    for index, state in enumerate(reward_states):\n",
    "        for action in range(len(actions)):\n",
    "            temp_dict[action] = reward_values[index]\n",
    "        rewards[state] = temp_dict.copy()\n",
    "    return rewards\n",
    "\n",
    "def populate_penalising_edges(rewards):\n",
    "    ''' \n",
    "    this has one hell of an edge case, where you could be in the cell which is on the perimeter\n",
    "\n",
    "    this is the sort of thing we will be handling later, and for now we will assume you cannot \n",
    "    have a reward cell on the perimeter \n",
    "    '''\n",
    "    for s in states:\n",
    "        if s not in rewards:\n",
    "            rewards[s] = {}\n",
    "\n",
    "        for action_index, (steps_right, steps_up) in enumerate(actions):\n",
    "            new_row = s[0] + steps_right\n",
    "            new_col = s[1] + steps_up\n",
    "\n",
    "            # Check if this move goes out of bounds\n",
    "            if (new_row < 0 or new_row >= len_y or\n",
    "                new_col < 0 or new_col >= len_x):\n",
    "                rewards[s][action_index] = -1\n",
    "            else:\n",
    "                 if action_index not in rewards[s]:\n",
    "                    rewards[s][action_index] = 0\n",
    "\n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = populate_reward_cells(actions, reward_states, reward_values)\n",
    "rewards = populate_penalising_edges(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 10, 1: 10, 2: 10, 3: 10}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewards[(8,7)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from MDP import GenericMDP\n",
    "mdp_solver = GenericMDP(states, actions, probabilities, rewards, 0.9, 20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.37700006,  1.75631032,  2.06727055,  2.41229572,  2.80728923,\n",
       "         3.25946591,  3.77589807,  4.29849387,  3.91585833,  3.35391213],\n",
       "       [ 1.6288284 ,  1.91035485,  2.26002193,  2.67269485,  3.1562562 ,\n",
       "         3.71824628,  4.37436645,  5.11011325,  4.56303896,  4.2851314 ],\n",
       "       [ 1.71978096,  2.02087277,  2.42486539,  2.93049844,  3.56180527,\n",
       "         4.2775603 ,  5.14283059,  6.22044812,  5.34133284,  5.05864124],\n",
       "       [ 1.61395853,  1.84994841,  2.14856498,  2.53089662,  3.54593743,\n",
       "         4.24858208,  4.94749848,  5.70788112,  6.18332563,  5.93171706],\n",
       "       [ 1.6494942 ,  1.88019617,  1.88185409, -1.77955188,  3.56652832,\n",
       "         4.75048039,  5.62562489,  6.58137558,  7.3889553 ,  6.96204048],\n",
       "       [ 1.88343777,  2.21568169,  2.66575438,  3.27308018,  4.60834409,\n",
       "         5.58863084,  6.60394126,  7.77199058,  8.91039452,  8.1533275 ],\n",
       "       [ 1.88438252,  2.19477089,  2.61039606,  3.22294645,  5.24379883,\n",
       "         6.50948493,  7.75893531,  9.17996279, 10.81287288,  9.5095379 ],\n",
       "       [ 1.80636709,  2.01350656,  1.58325021, -6.77955188,  4.86674633,\n",
       "         7.18263355,  8.83722659, 10.77855047, 13.22044812, 11.02654199],\n",
       "       [ 2.05235561,  2.39827241,  2.82104546,  3.37598478,  5.33227744,\n",
       "         6.57022207,  7.79998441,  9.20134566, 10.81715922,  9.43791646],\n",
       "       [ 2.15732168,  2.82293766,  3.42677746,  4.14350636,  5.01324156,\n",
       "         5.88766552,  6.84962524,  7.93852456,  9.00825903,  7.42486888]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary = mdp_solver._value_iteration()\n",
    "array = np.zeros((len_x, len_y))\n",
    "for item in dictionary:\n",
    "    array[item[0], item[1]] = dictionary[item]\n",
    "\n",
    "array.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, 0): np.float64(0.0),\n",
       " (0, 1): np.float64(0.0),\n",
       " (0, 2): np.float64(0.0),\n",
       " (0, 3): np.float64(0.0),\n",
       " (0, 4): np.float64(0.0),\n",
       " (0, 5): np.float64(0.0),\n",
       " (0, 6): np.float64(0.0),\n",
       " (0, 7): np.float64(0.0),\n",
       " (0, 8): np.float64(0.0),\n",
       " (0, 9): np.float64(0.0),\n",
       " (1, 0): np.float64(0.0),\n",
       " (1, 1): np.float64(0.0),\n",
       " (1, 2): np.float64(0.0),\n",
       " (1, 3): np.float64(0.0),\n",
       " (1, 4): np.float64(0.0),\n",
       " (1, 5): np.float64(0.0),\n",
       " (1, 6): np.float64(0.0),\n",
       " (1, 7): np.float64(0.0),\n",
       " (1, 8): np.float64(0.0),\n",
       " (1, 9): np.float64(0.0),\n",
       " (2, 0): np.float64(0.0),\n",
       " (2, 1): np.float64(0.0),\n",
       " (2, 2): np.float64(0.0),\n",
       " (2, 3): np.float64(0.0),\n",
       " (2, 4): np.float64(0.0),\n",
       " (2, 5): np.float64(0.0),\n",
       " (2, 6): np.float64(0.0),\n",
       " (2, 7): np.float64(0.0),\n",
       " (2, 8): np.float64(0.0),\n",
       " (2, 9): np.float64(0.0),\n",
       " (3, 0): np.float64(0.0),\n",
       " (3, 1): np.float64(0.0),\n",
       " (3, 2): np.float64(0.0),\n",
       " (3, 3): np.float64(0.0),\n",
       " (3, 4): np.float64(-5.0),\n",
       " (3, 5): np.float64(0.0),\n",
       " (3, 6): np.float64(0.0),\n",
       " (3, 7): np.float64(-10.0),\n",
       " (3, 8): np.float64(0.0),\n",
       " (3, 9): np.float64(0.0),\n",
       " (4, 0): np.float64(0.0),\n",
       " (4, 1): np.float64(0.0),\n",
       " (4, 2): np.float64(0.0),\n",
       " (4, 3): np.float64(0.0),\n",
       " (4, 4): np.float64(0.0),\n",
       " (4, 5): np.float64(0.0),\n",
       " (4, 6): np.float64(0.0),\n",
       " (4, 7): np.float64(0.0),\n",
       " (4, 8): np.float64(0.0),\n",
       " (4, 9): np.float64(0.0),\n",
       " (5, 0): np.float64(0.0),\n",
       " (5, 1): np.float64(0.0),\n",
       " (5, 2): np.float64(0.0),\n",
       " (5, 3): np.float64(0.0),\n",
       " (5, 4): np.float64(0.0),\n",
       " (5, 5): np.float64(0.0),\n",
       " (5, 6): np.float64(0.0),\n",
       " (5, 7): np.float64(0.0),\n",
       " (5, 8): np.float64(0.0),\n",
       " (5, 9): np.float64(0.0),\n",
       " (6, 0): np.float64(0.0),\n",
       " (6, 1): np.float64(0.0),\n",
       " (6, 2): np.float64(0.0),\n",
       " (6, 3): np.float64(0.0),\n",
       " (6, 4): np.float64(0.0),\n",
       " (6, 5): np.float64(0.0),\n",
       " (6, 6): np.float64(0.0),\n",
       " (6, 7): np.float64(0.0),\n",
       " (6, 8): np.float64(0.0),\n",
       " (6, 9): np.float64(0.0),\n",
       " (7, 0): np.float64(0.0),\n",
       " (7, 1): np.float64(0.0),\n",
       " (7, 2): np.float64(3.0),\n",
       " (7, 3): np.float64(0.0),\n",
       " (7, 4): np.float64(0.0),\n",
       " (7, 5): np.float64(0.0),\n",
       " (7, 6): np.float64(0.0),\n",
       " (7, 7): np.float64(0.0),\n",
       " (7, 8): np.float64(0.0),\n",
       " (7, 9): np.float64(0.0),\n",
       " (8, 0): np.float64(0.0),\n",
       " (8, 1): np.float64(0.0),\n",
       " (8, 2): np.float64(0.0),\n",
       " (8, 3): np.float64(0.0),\n",
       " (8, 4): np.float64(0.0),\n",
       " (8, 5): np.float64(0.0),\n",
       " (8, 6): np.float64(0.0),\n",
       " (8, 7): np.float64(10.0),\n",
       " (8, 8): np.float64(0.0),\n",
       " (8, 9): np.float64(0.0),\n",
       " (9, 0): np.float64(0.0),\n",
       " (9, 1): np.float64(0.0),\n",
       " (9, 2): np.float64(0.0),\n",
       " (9, 3): np.float64(0.0),\n",
       " (9, 4): np.float64(0.0),\n",
       " (9, 5): np.float64(0.0),\n",
       " (9, 6): np.float64(0.0),\n",
       " (9, 7): np.float64(0.0),\n",
       " (9, 8): np.float64(0.0),\n",
       " (9, 9): np.float64(0.0)}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
